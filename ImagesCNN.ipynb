{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import keras.backend as K\n",
    "import keras.utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import Input, Activation, Flatten, Dense\n",
    "from keras.layers import (concatenate)\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "data_image = \"C:\\\\Users\\\\Priit\\\\Dropbox\\\\Informaatika\\\\Magister\\\\Tehisn채rviv천rgud\\\\data\\\\train_images\"\n",
    "data_dir = \"C:\\\\Users\\\\Priit\\\\Dropbox\\\\Informaatika\\\\Magister\\\\Tehisn채rviv천rgud\\\\data\"\n",
    "\n",
    "label_column = \"AdoptionSpeed\"\n",
    "\n",
    "BATCH_SIZE = 256 * 2\n",
    "\n",
    "test_size = 0.2\n",
    "height, width = 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/christofhenkel/weighted-kappa-loss-for-keras-tensorflow\n",
    "def kappa_loss(y_pred, y_true, y_pow=2, eps=1e-3, N=5, bsize=BATCH_SIZE, name='kappa'):\n",
    "    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n",
    "        Args:\n",
    "            y_pred: 2D tensor or array, [batch_size, num_classes]\n",
    "            y_true: 2D tensor or array,[batch_size, num_classes]\n",
    "            y_pow: int,  e.g. y_pow=2\n",
    "            N: typically num_classes of the model\n",
    "            bsize: batch_size of the training or validation ops\n",
    "            eps: a float, prevents divide by zero\n",
    "            name: Optional scope/name for op_scope.\n",
    "        Returns:\n",
    "            A tensor with the kappa loss.\"\"\"\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom / (denom + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_dir, \"train.csv\"), sep=',')\n",
    "pet_ids = train[\"PetID\"]\n",
    "\n",
    "selected_columns = [\"Type\",\n",
    "                    \"Gender\",\n",
    "                    \"Color1\",\n",
    "                    \"Color2\",\n",
    "                    \"Color3\",\n",
    "                    \"MaturitySize\",\n",
    "                    \"FurLength\",\n",
    "                    \"Vaccinated\",\n",
    "                    \"Dewormed\",\n",
    "                    \"Sterilized\",\n",
    "                    \"Health\",\n",
    "                    \"State\",\n",
    "                    \"MaturitySize\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = train[label_column]\n",
    "\n",
    "# One-Hot-encode\n",
    "X = pd.get_dummies(train[selected_columns], columns=selected_columns)\n",
    "\n",
    "# Normalize:\n",
    "to_normalize = [\"Age\", \"Fee\", \"Quantity\"]\n",
    "for to_norm in to_normalize:\n",
    "     X[to_norm] = (train[to_norm] - train[to_norm].mean()) / train[to_norm].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14993, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "f_im_name = \"images.binary\"\n",
    "\n",
    "if not os.path.isfile(f_im_name):\n",
    "    \n",
    "    image_paths = [os.path.join(data_dir, data_image, pet_id + \"-1.jpg\") for pet_id in pet_ids]\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for path in tqdm(image_paths):\n",
    "        if os.path.isfile(path):\n",
    "            image = Image.open(path).convert(\"RGB\") \n",
    "            image.load()\n",
    "            image = np.asarray(image, dtype=\"int32\" )\n",
    "            image = resize(image, (height, width), anti_aliasing=True, mode='constant')\n",
    "        else:\n",
    "            image = np.random.rand(height, width, 3) * 255\n",
    "        \n",
    "        images.append(image)\n",
    "    \n",
    "    images = np.array(images)\n",
    "\n",
    "    # Standardize:\n",
    "    mean = np.mean(images)\n",
    "    std = np.std(images)\n",
    "\n",
    "    images_meanstd = (images - mean)/std\n",
    "    with open(f_im_name, 'wb') as handle:\n",
    "        pickle.dump(images_meanstd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open(f_im_name, 'rb') as handle:\n",
    "        images_meanstd = pickle.load(handle)\n",
    "\n",
    "print(images_meanstd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "X_train_img, X_test_img, X_train_else, X_test_else, y_train, y_test = train_test_split(images_meanstd, \n",
    "                                                                                       X, \n",
    "                                                                                       y, \n",
    "                                                                                       test_size=test_size,\n",
    "                                                                                       random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Priit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priit\\Anaconda3\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Priit\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9595 samples, validate on 2399 samples\n",
      "Epoch 1/15\n",
      "9595/9595 [==============================] - ETA: 22:03 - loss: 1.9800 - acc: 0.02 - ETA: 20:25 - loss: 1.7700 - acc: 0.15 - ETA: 19:28 - loss: 1.7385 - acc: 0.19 - ETA: 18:01 - loss: 1.7017 - acc: 0.20 - ETA: 16:45 - loss: 1.6670 - acc: 0.23 - ETA: 15:24 - loss: 1.6415 - acc: 0.24 - ETA: 14:02 - loss: 1.6261 - acc: 0.24 - ETA: 12:44 - loss: 1.6140 - acc: 0.25 - ETA: 11:26 - loss: 1.6099 - acc: 0.26 - ETA: 10:11 - loss: 1.6085 - acc: 0.26 - ETA: 8:58 - loss: 1.6030 - acc: 0.2754 - ETA: 7:47 - loss: 1.5956 - acc: 0.281 - ETA: 6:37 - loss: 1.5946 - acc: 0.285 - ETA: 5:26 - loss: 1.5921 - acc: 0.290 - ETA: 4:18 - loss: 1.5839 - acc: 0.294 - ETA: 3:08 - loss: 1.5788 - acc: 0.296 - ETA: 1:58 - loss: 1.5778 - acc: 0.298 - ETA: 50s - loss: 1.5751 - acc: 0.300 - 1560s 163ms/step - loss: 1.5774 - acc: 0.3018 - val_loss: 2.8822 - val_acc: 0.2280\n",
      "Epoch 2/15\n",
      "9595/9595 [==============================] - ETA: 20:17 - loss: 1.5958 - acc: 0.34 - ETA: 18:58 - loss: 1.5209 - acc: 0.37 - ETA: 17:48 - loss: 1.5400 - acc: 0.36 - ETA: 16:34 - loss: 1.5269 - acc: 0.36 - ETA: 15:35 - loss: 1.5305 - acc: 0.36 - ETA: 14:28 - loss: 1.5300 - acc: 0.35 - ETA: 13:05 - loss: 1.5254 - acc: 0.35 - ETA: 11:53 - loss: 1.5189 - acc: 0.35 - ETA: 10:45 - loss: 1.5245 - acc: 0.35 - ETA: 9:38 - loss: 1.5154 - acc: 0.3520 - ETA: 8:26 - loss: 1.5170 - acc: 0.349 - ETA: 7:20 - loss: 1.5202 - acc: 0.349 - ETA: 6:14 - loss: 1.5164 - acc: 0.350 - ETA: 5:08 - loss: 1.5180 - acc: 0.353 - ETA: 4:02 - loss: 1.5189 - acc: 0.352 - ETA: 2:57 - loss: 1.5144 - acc: 0.351 - ETA: 1:52 - loss: 1.5194 - acc: 0.350 - ETA: 48s - loss: 1.5163 - acc: 0.349 - 1532s 160ms/step - loss: 1.5122 - acc: 0.3501 - val_loss: 3.7273 - val_acc: 0.2830\n",
      "Epoch 3/15\n",
      "9595/9595 [==============================] - ETA: 20:20 - loss: 1.4781 - acc: 0.38 - ETA: 18:30 - loss: 1.4855 - acc: 0.37 - ETA: 17:11 - loss: 1.4769 - acc: 0.36 - ETA: 16:02 - loss: 1.4932 - acc: 0.34 - ETA: 14:47 - loss: 1.4999 - acc: 0.35 - ETA: 13:40 - loss: 1.5295 - acc: 0.35 - ETA: 12:36 - loss: 1.5190 - acc: 0.35 - ETA: 11:32 - loss: 1.5068 - acc: 0.35 - ETA: 10:27 - loss: 1.4994 - acc: 0.35 - ETA: 9:19 - loss: 1.4983 - acc: 0.3602 - ETA: 8:12 - loss: 1.4924 - acc: 0.360 - ETA: 7:12 - loss: 1.5038 - acc: 0.357 - ETA: 6:12 - loss: 1.5045 - acc: 0.358 - ETA: 5:08 - loss: 1.5038 - acc: 0.359 - ETA: 4:02 - loss: 1.5020 - acc: 0.360 - ETA: 2:57 - loss: 1.5051 - acc: 0.358 - ETA: 1:53 - loss: 1.5011 - acc: 0.359 - ETA: 48s - loss: 1.4980 - acc: 0.359 - 1509s 157ms/step - loss: 1.4976 - acc: 0.3607 - val_loss: 3.3167 - val_acc: 0.3001\n",
      "Epoch 4/15\n",
      "9595/9595 [==============================] - ETA: 17:05 - loss: 1.4498 - acc: 0.35 - ETA: 16:16 - loss: 1.4470 - acc: 0.36 - ETA: 15:17 - loss: 1.4915 - acc: 0.35 - ETA: 14:24 - loss: 1.4941 - acc: 0.36 - ETA: 13:25 - loss: 1.4896 - acc: 0.36 - ETA: 12:25 - loss: 1.4967 - acc: 0.36 - ETA: 11:27 - loss: 1.5255 - acc: 0.36 - ETA: 10:34 - loss: 1.5167 - acc: 0.36 - ETA: 9:40 - loss: 1.5137 - acc: 0.3674 - ETA: 8:53 - loss: 1.5084 - acc: 0.368 - ETA: 7:50 - loss: 1.5035 - acc: 0.369 - ETA: 6:48 - loss: 1.4904 - acc: 0.371 - ETA: 5:47 - loss: 1.4862 - acc: 0.370 - ETA: 4:47 - loss: 1.4825 - acc: 0.370 - ETA: 3:48 - loss: 1.4856 - acc: 0.372 - ETA: 2:48 - loss: 1.4853 - acc: 0.373 - ETA: 1:48 - loss: 1.4827 - acc: 0.374 - ETA: 46s - loss: 1.4759 - acc: 0.377 - 1502s 157ms/step - loss: 1.4743 - acc: 0.3783 - val_loss: 4.0354 - val_acc: 0.2830\n",
      "Epoch 5/15\n",
      "9595/9595 [==============================] - ETA: 20:46 - loss: 1.4936 - acc: 0.35 - ETA: 19:24 - loss: 1.4975 - acc: 0.37 - ETA: 18:20 - loss: 1.4670 - acc: 0.37 - ETA: 17:11 - loss: 1.4929 - acc: 0.38 - ETA: 16:07 - loss: 1.4821 - acc: 0.38 - ETA: 14:54 - loss: 1.4812 - acc: 0.38 - ETA: 13:37 - loss: 1.4793 - acc: 0.39 - ETA: 12:30 - loss: 1.4823 - acc: 0.39 - ETA: 11:24 - loss: 1.4722 - acc: 0.39 - ETA: 10:15 - loss: 1.4695 - acc: 0.38 - ETA: 9:05 - loss: 1.4579 - acc: 0.3887 - ETA: 7:53 - loss: 1.4602 - acc: 0.391 - ETA: 6:38 - loss: 1.4591 - acc: 0.391 - ETA: 5:24 - loss: 1.4629 - acc: 0.395 - ETA: 4:16 - loss: 1.4666 - acc: 0.394 - ETA: 3:08 - loss: 1.4605 - acc: 0.394 - ETA: 1:59 - loss: 1.4598 - acc: 0.394 - ETA: 50s - loss: 1.4531 - acc: 0.395 - 1571s 164ms/step - loss: 1.4562 - acc: 0.3945 - val_loss: 4.6052 - val_acc: 0.2835\n",
      "Epoch 6/15\n",
      "9595/9595 [==============================] - ETA: 18:26 - loss: 1.4167 - acc: 0.40 - ETA: 18:04 - loss: 1.4972 - acc: 0.38 - ETA: 17:01 - loss: 1.4448 - acc: 0.40 - ETA: 15:45 - loss: 1.4518 - acc: 0.40 - ETA: 14:40 - loss: 1.4698 - acc: 0.40 - ETA: 13:39 - loss: 1.4758 - acc: 0.40 - ETA: 12:43 - loss: 1.4630 - acc: 0.40 - ETA: 11:42 - loss: 1.4575 - acc: 0.40 - ETA: 10:43 - loss: 1.4518 - acc: 0.40 - ETA: 9:39 - loss: 1.4458 - acc: 0.4016 - ETA: 8:31 - loss: 1.4375 - acc: 0.403 - ETA: 7:26 - loss: 1.4434 - acc: 0.400 - ETA: 6:17 - loss: 1.4386 - acc: 0.401 - ETA: 5:09 - loss: 1.4352 - acc: 0.400 - ETA: 4:02 - loss: 1.4333 - acc: 0.397 - ETA: 2:57 - loss: 1.4405 - acc: 0.397 - ETA: 1:52 - loss: 1.4444 - acc: 0.396 - ETA: 47s - loss: 1.4435 - acc: 0.396 - 1468s 153ms/step - loss: 1.4436 - acc: 0.3977 - val_loss: 4.5353 - val_acc: 0.2830\n",
      "Epoch 7/15\n",
      "9595/9595 [==============================] - ETA: 16:50 - loss: 1.4484 - acc: 0.41 - ETA: 15:43 - loss: 1.4767 - acc: 0.41 - ETA: 15:05 - loss: 1.4289 - acc: 0.41 - ETA: 14:13 - loss: 1.4277 - acc: 0.43 - ETA: 13:20 - loss: 1.4240 - acc: 0.42 - ETA: 12:23 - loss: 1.4185 - acc: 0.41 - ETA: 11:26 - loss: 1.4205 - acc: 0.41 - ETA: 10:31 - loss: 1.4143 - acc: 0.41 - ETA: 9:33 - loss: 1.4238 - acc: 0.4169 - ETA: 8:34 - loss: 1.4186 - acc: 0.415 - ETA: 7:36 - loss: 1.4156 - acc: 0.415 - ETA: 6:37 - loss: 1.4147 - acc: 0.413 - ETA: 5:39 - loss: 1.4107 - acc: 0.413 - ETA: 4:40 - loss: 1.4195 - acc: 0.415 - ETA: 3:41 - loss: 1.4252 - acc: 0.414 - ETA: 2:42 - loss: 1.4259 - acc: 0.413 - ETA: 1:43 - loss: 1.4215 - acc: 0.412 - ETA: 43s - loss: 1.4229 - acc: 0.412 - 1385s 144ms/step - loss: 1.4226 - acc: 0.4116 - val_loss: 2.9641 - val_acc: 0.3710\n",
      "Epoch 8/15\n",
      "9595/9595 [==============================] - ETA: 17:27 - loss: 1.5623 - acc: 0.42 - ETA: 16:35 - loss: 1.5340 - acc: 0.41 - ETA: 15:33 - loss: 1.4606 - acc: 0.43 - ETA: 14:43 - loss: 1.4473 - acc: 0.41 - ETA: 13:39 - loss: 1.4436 - acc: 0.42 - ETA: 12:48 - loss: 1.4247 - acc: 0.41 - ETA: 11:40 - loss: 1.4333 - acc: 0.41 - ETA: 10:36 - loss: 1.4331 - acc: 0.41 - ETA: 9:33 - loss: 1.4309 - acc: 0.4180 - ETA: 8:33 - loss: 1.4283 - acc: 0.419 - ETA: 7:32 - loss: 1.4278 - acc: 0.418 - ETA: 6:32 - loss: 1.4205 - acc: 0.419 - ETA: 5:33 - loss: 1.4191 - acc: 0.421 - ETA: 4:34 - loss: 1.4173 - acc: 0.422 - ETA: 3:36 - loss: 1.4155 - acc: 0.421 - ETA: 2:38 - loss: 1.4183 - acc: 0.420 - ETA: 1:40 - loss: 1.4163 - acc: 0.421 - ETA: 42s - loss: 1.4143 - acc: 0.419 - 1355s 141ms/step - loss: 1.4142 - acc: 0.4180 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 9/15\n",
      "9595/9595 [==============================] - ETA: 17:34 - loss: 1.3898 - acc: 0.44 - ETA: 16:27 - loss: 1.3945 - acc: 0.43 - ETA: 15:38 - loss: 1.3959 - acc: 0.43 - ETA: 14:34 - loss: 1.3889 - acc: 0.44 - ETA: 13:29 - loss: 1.3793 - acc: 0.43 - ETA: 12:23 - loss: 1.3920 - acc: 0.43 - ETA: 11:23 - loss: 1.3964 - acc: 0.43 - ETA: 10:22 - loss: 1.3930 - acc: 0.44 - ETA: 9:23 - loss: 1.3936 - acc: 0.4399 - ETA: 8:23 - loss: 1.4009 - acc: 0.442 - ETA: 7:25 - loss: 1.3922 - acc: 0.439 - ETA: 6:26 - loss: 1.3962 - acc: 0.435 - ETA: 5:28 - loss: 1.3898 - acc: 0.435 - ETA: 4:31 - loss: 1.3902 - acc: 0.433 - ETA: 3:33 - loss: 1.3913 - acc: 0.433 - ETA: 2:36 - loss: 1.3949 - acc: 0.434 - ETA: 1:39 - loss: 1.3904 - acc: 0.433 - ETA: 42s - loss: 1.3922 - acc: 0.433 - 1327s 138ms/step - loss: 1.3942 - acc: 0.4347 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9595/9595 [==============================] - ETA: 16:52 - loss: 1.4631 - acc: 0.40 - ETA: 15:43 - loss: 1.3761 - acc: 0.42 - ETA: 14:48 - loss: 1.4009 - acc: 0.43 - ETA: 13:48 - loss: 1.3809 - acc: 0.44 - ETA: 12:55 - loss: 1.3892 - acc: 0.44 - ETA: 11:57 - loss: 1.3816 - acc: 0.45 - ETA: 11:01 - loss: 1.3777 - acc: 0.45 - ETA: 10:04 - loss: 1.3608 - acc: 0.45 - ETA: 9:08 - loss: 1.3599 - acc: 0.4581 - ETA: 8:11 - loss: 1.3658 - acc: 0.455 - ETA: 7:15 - loss: 1.3584 - acc: 0.460 - ETA: 6:19 - loss: 1.3557 - acc: 0.460 - ETA: 5:22 - loss: 1.3547 - acc: 0.461 - ETA: 4:26 - loss: 1.3543 - acc: 0.464 - ETA: 3:30 - loss: 1.3567 - acc: 0.463 - ETA: 2:34 - loss: 1.3623 - acc: 0.460 - ETA: 1:38 - loss: 1.3619 - acc: 0.460 - ETA: 41s - loss: 1.3618 - acc: 0.459 - 1322s 138ms/step - loss: 1.3633 - acc: 0.4582 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 11/15\n",
      "9595/9595 [==============================] - ETA: 17:59 - loss: 1.3333 - acc: 0.48 - ETA: 16:41 - loss: 1.3707 - acc: 0.47 - ETA: 15:45 - loss: 1.3430 - acc: 0.46 - ETA: 14:41 - loss: 1.3562 - acc: 0.46 - ETA: 13:42 - loss: 1.3632 - acc: 0.46 - ETA: 12:44 - loss: 1.3556 - acc: 0.46 - ETA: 11:43 - loss: 1.3672 - acc: 0.46 - ETA: 10:42 - loss: 1.3762 - acc: 0.46 - ETA: 9:42 - loss: 1.3584 - acc: 0.4668 - ETA: 8:41 - loss: 1.3485 - acc: 0.468 - ETA: 7:41 - loss: 1.3387 - acc: 0.469 - ETA: 6:41 - loss: 1.3286 - acc: 0.472 - ETA: 5:41 - loss: 1.3260 - acc: 0.473 - ETA: 4:42 - loss: 1.3286 - acc: 0.472 - ETA: 3:42 - loss: 1.3310 - acc: 0.474 - ETA: 2:43 - loss: 1.3306 - acc: 0.473 - ETA: 1:43 - loss: 1.3341 - acc: 0.470 - ETA: 44s - loss: 1.3340 - acc: 0.470 - 5783s 603ms/step - loss: 1.3325 - acc: 0.4707 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 12/15\n",
      "9595/9595 [==============================] - ETA: 17:23 - loss: 1.2724 - acc: 0.49 - ETA: 16:28 - loss: 1.2777 - acc: 0.48 - ETA: 15:29 - loss: 1.2798 - acc: 0.48 - ETA: 14:28 - loss: 1.2867 - acc: 0.48 - ETA: 13:55 - loss: 1.2919 - acc: 0.48 - ETA: 13:09 - loss: 1.3060 - acc: 0.48 - ETA: 12:11 - loss: 1.3053 - acc: 0.47 - ETA: 11:06 - loss: 1.2968 - acc: 0.47 - ETA: 10:00 - loss: 1.2946 - acc: 0.47 - ETA: 8:57 - loss: 1.2937 - acc: 0.4777 - ETA: 7:55 - loss: 1.3006 - acc: 0.473 - ETA: 6:53 - loss: 1.2970 - acc: 0.474 - ETA: 5:50 - loss: 1.2961 - acc: 0.477 - ETA: 4:49 - loss: 1.3087 - acc: 0.476 - ETA: 3:47 - loss: 1.3063 - acc: 0.475 - ETA: 2:48 - loss: 1.3065 - acc: 0.477 - ETA: 1:48 - loss: 1.3161 - acc: 0.473 - ETA: 46s - loss: 1.3153 - acc: 0.474 - 1443s 150ms/step - loss: 1.3118 - acc: 0.4768 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 13/15\n",
      "9595/9595 [==============================] - ETA: 17:42 - loss: 1.2061 - acc: 0.50 - ETA: 16:50 - loss: 1.2776 - acc: 0.50 - ETA: 15:45 - loss: 1.2379 - acc: 0.50 - ETA: 14:42 - loss: 1.2549 - acc: 0.50 - ETA: 13:39 - loss: 1.2529 - acc: 0.50 - ETA: 12:42 - loss: 1.2544 - acc: 0.50 - ETA: 11:40 - loss: 1.2501 - acc: 0.50 - ETA: 10:40 - loss: 1.2423 - acc: 0.50 - ETA: 9:40 - loss: 1.2375 - acc: 0.5111 - ETA: 8:40 - loss: 1.2372 - acc: 0.511 - ETA: 7:40 - loss: 1.2498 - acc: 0.510 - ETA: 6:40 - loss: 1.2546 - acc: 0.510 - ETA: 5:39 - loss: 1.2562 - acc: 0.509 - ETA: 4:39 - loss: 1.2566 - acc: 0.509 - ETA: 3:39 - loss: 1.2588 - acc: 0.507 - ETA: 2:40 - loss: 1.2604 - acc: 0.506 - ETA: 1:41 - loss: 1.2617 - acc: 0.507 - ETA: 43s - loss: 1.2662 - acc: 0.508 - 1350s 141ms/step - loss: 1.2667 - acc: 0.5092 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 14/15\n",
      "9595/9595 [==============================] - ETA: 16:30 - loss: 1.1256 - acc: 0.58 - ETA: 15:40 - loss: 1.1858 - acc: 0.54 - ETA: 14:41 - loss: 1.1759 - acc: 0.55 - ETA: 13:47 - loss: 1.1884 - acc: 0.55 - ETA: 12:49 - loss: 1.1805 - acc: 0.54 - ETA: 11:54 - loss: 1.1820 - acc: 0.54 - ETA: 10:57 - loss: 1.1940 - acc: 0.54 - ETA: 10:02 - loss: 1.1932 - acc: 0.54 - ETA: 9:06 - loss: 1.1957 - acc: 0.5421 - ETA: 8:12 - loss: 1.1949 - acc: 0.542 - ETA: 7:16 - loss: 1.1885 - acc: 0.539 - ETA: 6:19 - loss: 1.2008 - acc: 0.535 - ETA: 5:23 - loss: 1.1990 - acc: 0.534 - ETA: 4:26 - loss: 1.2196 - acc: 0.529 - ETA: 3:30 - loss: 1.2276 - acc: 0.528 - ETA: 2:34 - loss: 1.2240 - acc: 0.529 - ETA: 1:37 - loss: 1.2269 - acc: 0.529 - ETA: 41s - loss: 1.2267 - acc: 0.528 - 1312s 137ms/step - loss: 1.2286 - acc: 0.5266 - val_loss: 11.5561 - val_acc: 0.2830\n",
      "Epoch 15/15\n",
      "9595/9595 [==============================] - ETA: 16:26 - loss: 1.2199 - acc: 0.53 - ETA: 15:41 - loss: 1.1685 - acc: 0.54 - ETA: 14:45 - loss: 1.1724 - acc: 0.52 - ETA: 13:50 - loss: 1.1638 - acc: 0.52 - ETA: 12:53 - loss: 1.1496 - acc: 0.54 - ETA: 11:57 - loss: 1.1592 - acc: 0.54 - ETA: 10:59 - loss: 1.1873 - acc: 0.54 - ETA: 10:03 - loss: 1.1894 - acc: 0.54 - ETA: 9:07 - loss: 1.1896 - acc: 0.5382 - ETA: 8:10 - loss: 1.1880 - acc: 0.541 - ETA: 7:14 - loss: 1.1969 - acc: 0.538 - ETA: 6:18 - loss: 1.2085 - acc: 0.532 - ETA: 5:22 - loss: 1.2028 - acc: 0.531 - ETA: 4:25 - loss: 1.2009 - acc: 0.533 - ETA: 3:29 - loss: 1.2030 - acc: 0.531 - ETA: 2:33 - loss: 1.1988 - acc: 0.532 - ETA: 1:37 - loss: 1.1983 - acc: 0.534 - ETA: 41s - loss: 1.1994 - acc: 0.535 - 1310s 137ms/step - loss: 1.1971 - acc: 0.5345 - val_loss: 11.5561 - val_acc: 0.2830\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/49618986/neural-network-in-keras-with-two-different-input-types-images-and-values\n",
    "# https://www.learnopencv.com/keras-tutorial-transfer-learning-using-pre-trained-models/\n",
    "\n",
    "transfer = ResNet50(weights='imagenet', include_top=False, input_shape=(height, width, 3))\n",
    "\n",
    "# Freeze ResNet50\n",
    "for layer in transfer.layers[:-3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Inputs\n",
    "image_input = Input(shape=(height, width, 3))\n",
    "aux_input = Input(shape=(len(list(X_train_else)),))\n",
    "\n",
    "# Images:\n",
    "transfer = transfer(image_input)\n",
    "transfer = Dense(150, activation='relu')(transfer)\n",
    "flatten = Flatten()(transfer)\n",
    "\n",
    "# Aux input:\n",
    "x = Dense(150, activation='relu')(aux_input)\n",
    "x = Dense(250, activation='relu')(x)\n",
    "x = Dense(350, activation='relu')(x)\n",
    "\n",
    "# Merged:\n",
    "merge = concatenate([flatten, x])\n",
    "x = Dense(500)(merge)\n",
    "x = Dense(450, activation='relu')(x)\n",
    "x = Dense(100, activation='relu')(x)\n",
    "h = Dense(5)(x)\n",
    "\n",
    "# Predictions:\n",
    "predictions = Activation('softmax')(h)\n",
    "\n",
    "def kappa(y_true, y_pred):\n",
    "    y_pred = map(lambda x: np.argmax(x), y_pred) \n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "model = Model(inputs=[image_input, aux_input], outputs=predictions)\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit([X_train_img, X_train_else], \n",
    "                    keras.utils.to_categorical(y_train),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=15, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train_pred = [np.argmax(pred) for pred in model.predict([X_train_img, X_train_else])]\n",
    "test_predictions = [np.argmax(pred) for pred in model.predict([X_test_img, X_test_else])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa on train: 0.0\n",
      "Accuracy on train: 0.281\n",
      "________________\n",
      "Kappa on test: 0.0\n",
      "Accuracy on test: 0.2758\n"
     ]
    }
   ],
   "source": [
    "print(\"Kappa on train: {}\".format(round(cohen_kappa_score(y_train, train_pred, weights=\"quadratic\"), 4)))\n",
    "print(\"Accuracy on train: {}\".format(round(accuracy_score(y_train, train_pred), 4)))\n",
    "print(\"________________\")\n",
    "print(\"Kappa on test: {}\".format(round(cohen_kappa_score(y_test, test_predictions, weights=\"quadratic\"), 4)))\n",
    "print(\"Accuracy on test: {}\".format(round(accuracy_score(y_test, test_predictions), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "model.save('test_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (PetFinderNN)",
   "language": "python",
   "name": "pycharm-b185692d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
